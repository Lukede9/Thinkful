{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give Me Some Credit: Predicting loan repayment delinquencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Can we look at a candidate application and tell if they are going to pay back their loans on time with reasonable accuracy?\n",
    "\n",
    "Yes.\n",
    "\n",
    "### Introduction\n",
    "This dataset comes from a [2011 Kaggle competiton](https://www.kaggle.com/c/GiveMeSomeCredit).\n",
    "\n",
    "Here is the competition overview with one corrected typo:\n",
    "\n",
    "\"Banks play a crucial role in market economies. They decide who can get finance and on what terms and can make or break investment decisions. For markets and society to function, individuals and companies need access to credit. \n",
    "\n",
    "Credit scoring algorithms, which make a guess at the probability of default, are the method banks use to determine whether or not a loan should be granted. This competition requires participants to improve on the state of the art in credit scoring, by predicting the probability that somebody will experience financial distress in the next two years.\n",
    "\n",
    "The goal of this competition is to build a model that (lenders) can use to help make the best financial decisions.\"\n",
    "\n",
    "Top scores on the leaderboard were roughly .86.\n",
    "\n",
    "\n",
    "All Links:\n",
    "[2011 Kaggle competiton info](https://www.kaggle.com/c/GiveMeSomeCredit)\n",
    "[Google Slides Presentation Link](https://docs.google.com/presentation/d/1N7F6ZImFCXYkms8WacUt9lX4yIeLGt6-isRWhdfcNnE/edit?usp=sharing)\n",
    "[Data Exploration, Feature Engineering, Feature Selection](https://github.com/Lukede9/Thinkful/blob/master/Bootcamp/Unit%203/Capstone/Capstone%20EDA.ipynb)\n",
    "[See Models Here](https://github.com/Lukede9/Thinkful/blob/master/Bootcamp/Unit%203/Capstone/Capstone%20Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration Summary\n",
    "A link to my data exploration and explanations of my decisions can be found [here.](https://github.com/Lukede9/Thinkful/blob/master/Bootcamp/Unit%203/Capstone/Capstone%20EDA.ipynb) Summary:\n",
    "\n",
    "1. Are there any entries in the data that do not make sense (e.g. Monthly Income of 1 billion)?\n",
    "2. How does a change in age impact the likelihood to default on the loans? What about the other variables?\n",
    "3. Is any of the information we collected redundant?\n",
    "\n",
    "#### Missing Data\n",
    "The was missing data in the monthly income and number of dependents columns.\n",
    "- Roughly 20% of borrowers did not report monthly income.\n",
    "- Roughly 2.5% did not report their # of dependents.\n",
    "\n",
    "Many variables had strange outliers. These outliers had erratic relationships to the outcome variable. These include:\n",
    "- age < 21\n",
    "- NumberOfTime30-59DaysPastDueNotWorse > 13\n",
    "- NumberOfTime60-89DaysPastDueNotWorse > 11\n",
    "- NumberOfTimes90DaysLate > 17\n",
    "- RevolvingUtilizationOfUnsecuredLines > 2\n",
    "- NumberRealEstateLoansOrLines > 30\n",
    "- DebtRatio > 2\n",
    "- MonthlyIncome > 10000\n",
    "- NumberOfDependents > 6\n",
    "\n",
    "#### Multi-collinearity\n",
    "- The three columns about lateness are highly redundant.\n",
    "- The number of lines and loans has a correlation with number of real estate lines and loans\n",
    "\n",
    "#### Potential Data Transformations\n",
    "Taking the square root of certain variables leads to more normal distributions. This includes:\n",
    "- 'NumberOfOpenCreditLinesAndLoans'\n",
    "- 'NumberRealEstateLoansOrLines'\n",
    "- 'combined_lines'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each variable had a point where the data became too sparse to tell us anything about the correlations. I wrote functions to reduce the scale of the outliers.\n",
    "- The result is that the entries are still in the same order, which is helpful for random forest. At the same time, some of the outlier values were thousands of times larger than the average entries in a given column. Now they are more similar, which should help the logistic regression classifier.\n",
    "- I imputed the missing Monthly Incomes based on the age of the applicant.\n",
    "- I imputed the missing number of dependents to 0, the mode.\n",
    "- After this, I did what I could to make the features more normally distributed using simple mathematical transformations. The goal was to feed more normal data to the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I selected features with the goal of reducing multicollinearity within my variables. In doing so, many of the newly engineered features were rejected.\n",
    "\n",
    "- The helpful new feature that survived this filtering was called 'bi_combined_lates'. It marks whether the applicant has been late even once on a payment in the past two years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Tuning\n",
    "\n",
    "The models that I will be testing out are Logistic Regression, K-Nearest Neighbors, Random Forest, and Gradient Boosting. I chose these because they will still perform with non-normal data.\n",
    "\n",
    "The measure of performance is the area under the ROC curve.\n",
    "\n",
    "\n",
    "[See Models Here](https://github.com/Lukede9/Thinkful/blob/master/Bootcamp/Unit%203/Capstone/Capstone%20Models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Default Params</th>\n",
       "      <th>Tuned Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogR</th>\n",
       "      <td>0.809843</td>\n",
       "      <td>0.8192657488267187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.716889</td>\n",
       "      <td>Takes too long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rfc</th>\n",
       "      <td>0.775778</td>\n",
       "      <td>0.8462978738187573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gbc</th>\n",
       "      <td>0.868585</td>\n",
       "      <td>0.868585127643474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Default Params        Tuned Params\n",
       "LogR        0.809843  0.8192657488267187\n",
       "knn         0.716889      Takes too long\n",
       "rfc         0.775778  0.8462978738187573\n",
       "gbc         0.868585   0.868585127643474"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.read_csv(\"capstone_model_results.csv\", index_col=0)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression was promising but did not improve much from parameter tuning.\n",
    "\n",
    "The K-Nearest Neighbors model was never promising, never improved, and would spend your whole life running if you let it.\n",
    "\n",
    "Random Forest does better and better the more estimators you feed it, but it takes longer to run and caps off a bit shy of what the gradient booster could do.\n",
    "\n",
    "The Gradient Boosting Classifier does take a fair amount of time to run, but it practically matches the best kaggle scores while on default parameters. Tuning the parameters does not seem to make a difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaning institutions can always use a good credit-scoring algorithm in order to assess risks, choose whether or not to approve a credit line/loan, and to determine the rates they will offer to applicants.\n",
    "\n",
    "The models I have developed do just that. They determine the likelihood that the applicant will default on the loan in the next two years.\n",
    "\n",
    "The most successful model scored .868 under the ROC curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
