{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (0,19,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "y2015 = pd.read_csv(\n",
    "    'LoanStats3d.csv',\n",
    "    skipinitialspace=True,\n",
    "    header=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ID and Interest Rate to numeric.\n",
    "y2015['id'] = pd.to_numeric(y2015['id'], errors='coerce')\n",
    "y2015['int_rate'] = pd.to_numeric(y2015['int_rate'].str.strip('%'), errors='coerce')\n",
    "\n",
    "# Drop other columns with many unique variables\n",
    "y2015.drop(['url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc', 'policy_code', 'member_id', 'id'], 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2015 = y2015[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment starts here\n",
    "\n",
    "Goal is to remove as much data as possible while still staying above an average of .9 for a 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the continuous variables from the original DataFrame\n",
    "\n",
    "cont_y2015 = y2015.select_dtypes(include=['float64'])\n",
    "cat_y2015 = y2015.select_dtypes(exclude=['float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dti_joint                         420586\n",
       "annual_inc_joint                  420584\n",
       "il_util                           402478\n",
       "mths_since_rcnt_il                400285\n",
       "inq_last_12m                      399723\n",
       "open_acc_6m                       399723\n",
       "open_il_6m                        399723\n",
       "open_il_12m                       399723\n",
       "open_il_24m                       399723\n",
       "total_bal_il                      399723\n",
       "open_rv_12m                       399723\n",
       "open_rv_24m                       399723\n",
       "all_util                          399723\n",
       "inq_fi                            399723\n",
       "total_cu_tl                       399723\n",
       "max_bal_bc                        399723\n",
       "mths_since_last_record            346680\n",
       "mths_since_recent_bc_dlq          312495\n",
       "mths_since_last_major_derog       298366\n",
       "mths_since_recent_revol_delinq    269358\n",
       "mths_since_last_delinq            203962\n",
       "mths_since_recent_inq              44599\n",
       "num_tl_120dpd_2m                   19230\n",
       "mo_sin_old_il_acct                 12254\n",
       "percent_bc_gt_75                    4239\n",
       "bc_util                             4227\n",
       "bc_open_to_buy                      3963\n",
       "mths_since_recent_bc                3798\n",
       "num_rev_accts                          1\n",
       "annual_inc                             0\n",
       "                                   ...  \n",
       "tot_coll_amt                           0\n",
       "tot_cur_bal                            0\n",
       "total_bal_ex_mort                      0\n",
       "tot_hi_cred_lim                        0\n",
       "tax_liens                              0\n",
       "pub_rec_bankruptcies                   0\n",
       "pct_tl_nvr_dlq                         0\n",
       "num_tl_op_past_12m                     0\n",
       "num_tl_90g_dpd_24m                     0\n",
       "num_tl_30dpd                           0\n",
       "num_sats                               0\n",
       "num_rev_tl_bal_gt_0                    0\n",
       "num_op_rev_tl                          0\n",
       "num_il_tl                              0\n",
       "num_bc_tl                              0\n",
       "num_bc_sats                            0\n",
       "num_actv_rev_tl                        0\n",
       "num_actv_bc_tl                         0\n",
       "num_accts_ever_120_pd                  0\n",
       "mort_acc                               0\n",
       "mo_sin_rcnt_tl                         0\n",
       "mo_sin_rcnt_rev_tl_op                  0\n",
       "mo_sin_old_rev_tl_op                   0\n",
       "delinq_amnt                            0\n",
       "chargeoff_within_12_mths               0\n",
       "avg_cur_bal                            0\n",
       "acc_open_past_24mths                   0\n",
       "total_rev_hi_lim                       0\n",
       "total_bc_limit                         0\n",
       "loan_amnt                              0\n",
       "Length: 84, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Figure out where what's going on with those NaNs\n",
    "\n",
    "y2015_na = cont_y2015.isnull().sum(axis = 0)\n",
    "y2015_na.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with loads of NaNs. Caution, some NaNs may be meaningful predictors.\n",
    "cont_y2015 = cont_y2015.dropna(thresh=len(cont_y2015) - 100000, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining NaNs were negligible. Dropped 'em.\n",
    "cont_y2015.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I rescale, I should get the corresponding rows from the categorical and outcome variables saved in a dataframe\n",
    "then after I rescale the continuous and get dummies for the categorical, I can rejoin them for the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code saves the categoricl data that goes with the data that I didn't drop above.\n",
    "cat_y2015_corresponding = cat_y2015.iloc[list(cont_y2015.index), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescaling the continuous variables. Potential issue could be the bankruptcies. Maybe make that one categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "rescaled_y2015 = scaler.fit_transform(cont_y2015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of the columns with lower than .01 variance\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=.01)\n",
    "filtered_rescaled_y2015 = selector.fit_transform(rescaled_y2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing it to a dataframe so I can rejoin it to my categorical dummies in future\n",
    "filtered_rescaled_y2015 = pd.DataFrame(filtered_rescaled_y2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dummies because it is currently categorical, removing loan_status because that's cheating.\n",
    "dummies_cat_y2015_corresponding = pd.get_dummies(cat_y2015_corresponding.drop('loan_status', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the indices so that I can concatenate these back together nicely.\n",
    "dummies_cat_y2015_corresponding = dummies_cat_y2015_corresponding.reset_index()\n",
    "filtered_rescaled_y2015 = filtered_rescaled_y2015.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X and Y for the model.\n",
    "X = pd.concat([dummies_cat_y2015_corresponding, filtered_rescaled_y2015], axis=1)\n",
    "Y = cat_y2015_corresponding['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97827537, 0.97900049, 0.97989964, 0.98068277, 0.94161156,\n",
       "       0.97659105, 0.91019319, 0.97731624, 0.97862095, 0.43045282])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "cross_val_score(rfc, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132644079999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, we dropped a lot of data and it is still over 90% accurate\n",
    "np.mean([0.97827537, 0.97900049, 0.97989964, 0.98068277, 0.94161156,\n",
    "       0.97659105, 0.91019319, 0.97731624, 0.97862095, 0.43045282])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dropped some data that might have been very useful. Specifically the data that says how many months it has been since last delinquency. This was because there were many NaN values. I suspect that the NaN values meant that they had never had any delinquencies. And this could be a valuable predictor. I may try it but putting in a huge value to represent never having any delinquencies.\n",
    "\n",
    "Also, there are some pieces of continuous data that might do better as categorical data. For excample the number of bankruptcies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
